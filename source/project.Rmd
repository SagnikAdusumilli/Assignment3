---
title: "ML 1000 - Project"
author: "Ali El-Sharif, Neha Panchal, Sagnik Adusumilli, Sarmad Shubber"
due date: "March 22, 2019"
output: word_document
---


# Data Analysis

Source of data: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

Extracted from https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#


## Data Dictionary

20 input variables and 1 output variable (desired target)

Column Name            | Column Description  
-----------------------| -------------------
age                    | age of the customer (numeric)
job                    | type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
marital                | marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed) 
education              | education level of the customer (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
default                | has credit in default? (categorical: 'no','yes','unknown')
housing                | has housing loan? (categorical: 'no','yes','unknown')
loan                   | has personal loan? (categorical: 'no','yes','unknown')
contact                | contact communication type (categorical: 'cellular','telephone')
month                  | last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
day_of_week            | last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
duration               | last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model
campaign               | number of contacts performed during this campaign and for this client (numeric, includes last contact)
pdays                  | number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
previous               | number of contacts performed before this campaign and for this client (numeric)
poutcome               | outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
emp.var.rate           | employment variation rate - quarterly indicator (numeric)
cons.price.idx         | consumer price index - monthly indicator (numeric) 
cons.conf.idx          | consumer confidence index - monthly indicator (numeric) 
euribor3m              | euribor 3 month rate - daily indicator (numeric)
nr.employed            | number of employees - quarterly indicator (numeric)
y                      | has the client subscribed a term deposit? (binary: 'yes','no')

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#import packages;
#library(dplyr)
#library(reshape2)
#library(ggplot2)
#library(Hmisc)
#library(corrplot)
#library(mice)
#library(VIM)
#library(pROC)
#library(caret)
#library(sqldf)

# Clean all variables that might be left by other scripts
rm(list=ls(all=TRUE))

```

```{r bankData, include=FALSE}
# Read data
bankData = read.csv("../data/bank-additional-full.csv", header = TRUE, na.strings =  c("NA","","#NA"), sep=";")

head(bankData)
```


## Data Exploartion

Summary:
```{r summary, echo=FALSE}

summary(bankData)

```

Structure:
```{r structure, echo=FALSE}

str(bankData)

```

Missing Data:
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Check missing values
sort(colSums(is.na(bankData)), decreasing = T)

```
Our dataset does not have any rows with columns that contain NA, so no rows will be omitted and no data imputing is necessary.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#skip all the data with missing values (no missing values were found in this case)
#bankData <- na.omit(bankData)

```

Correlation matrix:
```{r matrix, fig.width=5, fig.height=5, echo = FALSE, warning = FALSE, message = FALSE}
# we can only Create a correlation matrix with only numeric data, therefore we are going to use sapply to only get numeric data
# sapply is applying a fuction over list of vector
numericData <- bankData[sapply(bankData,is.numeric)]
matrix <- cor(numericData, use="pairwise.complete.obs")

library(corrplot)
corrplot(matrix, type = "lower", method = "circle", order="hclust", tl.srt = 45, tl.cex = 0.7)

```
Distribution of important features:
```{r feature_distribution, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" Distribution of Important Features"}

library(dplyr)
tmp = bankData %>% filter(complete.cases(.))

library(RColorBrewer) # color palettes
mainPalette = brewer.pal(8,"Dark2") # pick palettes 

library(ggplot2)
p1 = tmp %>% ggplot(aes(x=age)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) 

p2 = tmp %>% ggplot(aes(x=duration)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) 

p3 = tmp %>% ggplot(aes(x=campaign)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2) 

p4 = tmp %>% ggplot(aes(x=pdays)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) 

p5 = tmp %>% ggplot(aes(x=previous)) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2) 

p6 = tmp %>% ggplot(aes(x=emp.var.rate)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2) 

p7 = tmp %>% ggplot(aes(x=cons.price.idx)) + geom_density(fill=mainPalette[7], colour=mainPalette[7], alpha = 0.2) 

p8 = tmp %>% ggplot(aes(x=cons.conf.idx)) + geom_density(fill=mainPalette[8], colour=mainPalette[8], alpha = 0.2) 

p9 = tmp %>% ggplot(aes(x=euribor3m)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) 

p10 = tmp %>% ggplot(aes(x=nr.employed)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) 


library(gridExtra) # arrange grids
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10)
rm(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,tmp)
```


## Data Preparation and Unsupervised Learning (Clustering)

```{r}
#Encode columns in order to perform clustering on categorical values 
bankData1he <- bankData
#install.packages(ade4)
#One-hot-encoding of categorical features:
library(ade4)

categorical_columns = c('job', 'marital', 'education', 'default', 'housing', 
             'loan', 'contact', 'month', 'day_of_week', 'poutcome')
for (f in categorical_columns){
  df_all_dummy = acm.disjonctif(bankData1he[f])
  bankData1he[f] = NULL
  bankData1he = cbind(bankData1he, df_all_dummy)
}

str(bankData1he)
head(bankData1he)


# Remove y column (column 11) before scaling and clustering
#scale the variables
scaled_bd <- scale(bankData1he[,-c(11)])

#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
wss <- sapply(1:k.max, 
              function(k){kmeans(scaled_bd, k, nstart=50,iter.max = 15 )$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

# k = 3, from the above plot we picked the elbow to be k=3 meaning 3 clusters
library(cluster)
clarax <- clara(scaled_bd, 3, metric = "jaccard", stand = TRUE, 
      samples = 5, pamLike = TRUE)

clarax
# cluster sizes: 19144, 9339, 12705

bankDataWithCluster <- cbind(bankData, cluster = clarax$clustering)
#head(bankDataWithCluster, n = 4)

# Medoids
clarax$medoids
# The mediods are 4152, 21730, 13128

# plot the cluster solution
library(fpc)
plotcluster(scaled_bd, clarax$clustering)

# display data around the mediods for each cluster
bankDataWithCluster[4140:4160,]
bankDataWithCluster[21720:21740,]
bankDataWithCluster[13120:13140,]

cluster1 <- bankDataWithCluster[bankDataWithCluster$cluster==1,]
cluster2 <- bankDataWithCluster[bankDataWithCluster$cluster==2,]
cluster3 <- bankDataWithCluster[bankDataWithCluster$cluster==3,]

#Summary by cluster
summary(cluster1)
summary(cluster2)
summary(cluster3)

```


## Clustering Analysis

To learn more about each cluster, we will run a Random Forest and get the important features (columns) of the cluster. We will also run a Decision Tree to know which are the most important features (columns).

```{r}
######################################## Random Forest on cluster1 - most important features
#19144 rows

# Set a random seed
set.seed(754)

library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster1)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: age, euribor3m
#3rd feature: job
#4th feature: nr.employed

```

```{r}
##################################### Decision Tree on cluster1 - most important features
#19144 rows

set.seed(1984)

library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster1,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, pdays, poutcome, previous

```

```{r}
######################################## Random Forest on cluster2 - most important features
#9339 rows

# Set a random seed
set.seed(454)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: euribor3m, age
#3rd feature: job
#4th feature: nr.employed

```

```{r}
##################################### Decision Tree on cluster2 - most important features
#9339 rows

set.seed(1484)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, poutcome, pdays

```

```{r}
######################################## Random Forest on cluster3 - most important features
#12705 rows

# Set a random seed
set.seed(760)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster3)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: age, euribor3m
#3rd feature: job
#4th feature: nr.employed
```

```{r}
##################################### Decision Tree on cluster3 - most important features
#12705 rows

set.seed(1254)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster3,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, pdays, poutcome, previous

```

## Supervised Learning (Prediction Models)

```{r}
########################### Supervised Learning - prediction models

######################################## Random Forest
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

# Set a random seed
set.seed(754)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = train)

# Show model error
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()


# Predict using the test set
prediction <- predict(rf_model, test)

solution <- data.frame(test, subscribed = prediction)
#solution

library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.6326

library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.6326

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")

```

```{r}
##################################### Decision Tree
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

set.seed(1934)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = train,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

# Predict using the test set
prediction <- predict(dt_model, test)

solution <- data.frame(test, subscribed = prediction)
solution$subscribed <- ifelse(solution$subscribed.yes > solution$subscribed.no, "yes", "no")

#solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#nrow(solution[solution$y=='no',])
#Balanced Accuracy: 0.5914

#library(pROC)
auc(as.numeric(solution$y), as.numeric(factor(solution$subscribed)))
#Area under the curve: 0.5914

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(factor(solution$subscribed))), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")

```

```{r}
################################# Naive Bayes
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

library(e1071)

nb_model <- naiveBayes(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data=train)

# Predict using the test set
prediction <- predict(nb_model, test)

solution <- data.frame(test, subscribed = prediction)
#solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.7129

#library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.7129

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")

```

```{r, include=FALSE}
################################### Support vector machine (SVM)
# SVM takes over 30 minutes to run so we are excluding it
##bankDataWithCluster1he <- bankDataWithCluster
#install.packages(ade4)
#One-hot-encoding of categorical features because SVM only accepts numerical columns:
#library(ade4)
#library(data.table)
##categorical_columns = c('job', 'marital', 'education', 'default', 'housing', 
##                        'loan', 'contact', 'month', 'day_of_week', 'poutcome')
##for (f in categorical_columns){
##  df_all_dummy = acm.disjonctif(bankDataWithCluster1he[f])
##  bankDataWithCluster1he[f] = NULL
##  bankDataWithCluster1he = cbind(bankDataWithCluster1he, df_all_dummy)
##}

##bankDataWithCluster1he$y <- ifelse(bankDataWithCluster1he$y == "yes", 1, 0)
##bankDataWithCluster1he$y <- factor(bankDataWithCluster1he$y)

#remove duration as per UCI website
##bankDataWithCluster1he$duration <- NULL

#remove cluster column 12
##bankDataWithCluster1he$cluster <- NULL

##nrow(bankDataWithCluster1he[bankDataWithCluster1he$default.yes==1,]) #3
#remove default.yes column 38 because it is nearly all 0's and SVM doesn't allow that
##bankDataWithCluster1he$default.yes <- NULL

##nrow(bankDataWithCluster1he[bankDataWithCluster1he$education.illiterate==1,]) #19
#remove education.illiterate column 32 because it is nearly all 0's and SVM doesn't allow that
##bankDataWithCluster1he$education.illiterate <- NULL

##row_count <- nrow(bankDataWithCluster1he)
##shuffled_rows <- sample(row_count)
##train <- bankDataWithCluster1he[head(shuffled_rows,floor(row_count*0.75)),]
##test <- bankDataWithCluster1he[tail(shuffled_rows,floor(row_count*0.25)),]

#library(caret)
##trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
##set.seed(3233)

##svm_Linear <- train(y ~ .,
##                          data = train,
##                          method = "svmLinear",
##                          trControl=trctrl,
##                          preProcess = c("center", "scale"),
##                          tuneLength = 10)

# Predict using the test set
##prediction <- predict(svm_Linear, newdata = test)
##prediction

##solution <- data.frame(test, subscribed = prediction)
##solution

#library(caret)
##confusionMatrix(factor(solution$subscribed), solution$y)

#library(pROC)
##auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.5997

##roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col ##= "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")

```


## Prediction Models Analysis

The higher the area under the curve (AUC), the better the prediction model.
We tried 4 classification models: Random Forest, Decision Tree, Naive Bayes, and Support Vector Machine (SVM). SVM takes over 30 minutes to run so we are excluding its results.
The order of models from highest AUC to lowest is:
1. Naive Bayes    (AUC: 0.7129)
2. Random Forest  (AUC: 0.6326)
3. Decision Tree  (AUC: 0.5914)