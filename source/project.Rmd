---
title: "ML 1000 - Project"
author: "Ali El-Sharif, Neha Panchal, Sagnik Adusumilli, Sarmad Shubber"
due date: "March 22, 2019"
output: word_document
---


## Abstract

TBD


## Background 

TBD


## Objective

TBD


# Data Analysis

Source of data: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

Extracted from https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#


## Data Dictionary

Column Name            | Column Description  
-----------------------| ------------------- 
age                    | Age of the customer
job                    | Type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
marital                | Marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed) 
education              | Education level of the customer*
default                | has credit in default(categorical: 'no','yes','unknown')
balance                | ??
housing                | has housing loan  (categorical: 'no','yes','unknown')
loan                   | has personal loan (yes/no)  
contract               | contact communication type (categorical: 'cellular','telephone') 
day                    | ??
month                  | month the customer was contrained previously in the year
duration               |  last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously 
campaign               |  number of contacts performed during this campaign and for this client (numeric, includes last contact)
pdays                  | number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
previous               | number of contacts performed before this campaign and for this client
poutcome               | outcome of the previous marketing campaign
y                      | has the client subscribed a term deposit? (binary: 'yes','no')

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#import packages;
#library(dplyr)
#library(reshape2)
#library(ggplot2)
#library(Hmisc)
#library(corrplot)
#library(mice)
#library(VIM)
#library(pROC)
#library(caret)
#library(sqldf)

# Clean all variables that might be left by other scripts
rm(list=ls(all=TRUE))

```

```{r bankData, include=FALSE}
# Read data
bankData = read.csv("../data/bank-additional-full.csv", header = TRUE, na.strings =  c("NA","","#NA"), sep=";")

head(bankData)
```


## Data Exploartion

Summary:
```{r summary, echo=FALSE}

summary(bankData)

```


Structure:
```{r structure, echo=FALSE}

str(bankData)

```


Creating a correlation matrix

```{r matrix, fig.width=10, fig.height=10, echo = FALSE, warning = FALSE, message = FALSE}
# we can only Create a correlation matrix with only numeric data, therefore we are going to use sapply to only get numeric data
# sapply is applying a fuction over list of vector
numericData <- bankData[sapply(bankData,is.numeric)]
matrix <- cor(numericData, use="pairwise.complete.obs")

library(corrplot)
corrplot(matrix, type = "lower", method = "circle", order="hclust", tl.srt = 45, tl.cex = 0.7)

```


## Missing Data
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Check missing values
sort(colSums(is.na(bankData)), decreasing = T)

```
Our dataset does not have any rows with columns that contain N/A.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#skip all the data with missing values (no missing values were found in this case)
#bankData <- na.omit(bankData)

```


```{r feature_distribution, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" Distribution of Important Features"}

library(dplyr)
tmp = bankData %>% filter(complete.cases(.))

library(RColorBrewer) # color palettes
mainPalette = brewer.pal(8,"Dark2") # pick palettes 

library(ggplot2)
p1 = tmp %>% ggplot(aes(x=age)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) #+ scale_x_log10()

p2 = tmp %>% ggplot(aes(x=duration)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) #+ scale_x_log10()

p3 = tmp %>% ggplot(aes(x=campaign)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2) #+ scale_x_log10()

p4 = tmp %>% ggplot(aes(x=pdays)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) #+ scale_x_log10()

p5 = tmp %>% ggplot(aes(x=previous)) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2) #+ scale_x_log10()

p6 = tmp %>% ggplot(aes(x=emp.var.rate)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2) #+ scale_x_log10()

p7 = tmp %>% ggplot(aes(x=cons.price.idx)) + geom_density(fill=mainPalette[7], colour=mainPalette[7], alpha = 0.2) #+ scale_x_log10()

p8 = tmp %>% ggplot(aes(x=cons.conf.idx)) + geom_density(fill=mainPalette[8], colour=mainPalette[8], alpha = 0.2) #+ scale_x_log10() # this plot looks strange

p9 = tmp %>% ggplot(aes(x=euribor3m)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) #+ scale_x_log10()

p10 = tmp %>% ggplot(aes(x=nr.employed)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) #+ scale_x_log10()


library(gridExtra) # arrange grids
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10)
rm(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,tmp)
```

## Insights from Data explorations


 

## Data Preparation

```{r}
bankData1he <- bankData
#install.packages(ade4)
#One-hot-encoding features:
library(ade4)
#library(data.table)
ohe_feats = c('job', 'marital', 'education', 'default', 'housing', 
             'loan', 'contact', 'month', 'day_of_week', 'poutcome')
for (f in ohe_feats){
  df_all_dummy = acm.disjonctif(bankData1he[f])
  bankData1he[f] = NULL
  bankData1he = cbind(bankData1he, df_all_dummy)
}

str(bankData1he)
head(bankData1he)



# Remove y column before clustering
#scale the variables
scaled_bd <- scale(bankData1he[,-11])

#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
data <- scaled_bd
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

# k = 4
library(cluster)
clarax <- clara(bankData1he[,-11], 4, metric = "manhattan", stand = TRUE, 
      samples = 5, pamLike = TRUE)

clarax
# cluster sizes: 15375, 8373, 12472, 4968

dd <- cbind(bankData, cluster = clarax$clustering)
head(dd, n = 4)

# Medoids
clarax$medoids
# The mediods are 631, 14877, 30462, 24381

# plot the cluster solution
library(fpc)
plotcluster(scaled_bd, clarax$clustering)

dd[610:650,]
dd[14860:14900,]
dd[30440:30480,]
dd[24360:24400,]

cluster1 <- dd[dd$cluster==1,]
cluster2 <- dd[dd$cluster==2,]
cluster3 <- dd[dd$cluster==3,]
cluster4 <- dd[dd$cluster==4,]

cluster1yes <- cluster1[cluster1$y=='yes',]
nrow(cluster1yes) #851
cluster1no <- cluster1[cluster1$y=='no',]
nrow(cluster1no) #14524

cluster2yes <- cluster2[cluster2$y=='yes',]
nrow(cluster2yes) #492
cluster2no <- cluster2[cluster2$y=='no',]
nrow(cluster2no) #7881

cluster3yes <- cluster3[cluster3$y=='yes',]
nrow(cluster3yes) #2832
cluster3no <- cluster3[cluster3$y=='no',]
nrow(cluster3no) #9640

cluster4yes <- cluster4[cluster4$y=='yes',]
nrow(cluster4yes) #465
cluster4no <- cluster4[cluster4$y=='no',]
nrow(cluster4no) #4503

```

 

## Data Imputing


 

## Feature Selection


 

## Modeling

```{r}
######################################## Random Forest
#15375
row_count <- nrow(cluster1)
shuffled_rows <- sample(row_count)
cluster1train <- cluster1[head(shuffled_rows,floor(row_count*0.75)),]
cluster1test <- cluster1[tail(shuffled_rows,floor(row_count*0.25)),]

# Set a random seed
set.seed(754)

library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster1train)

# Show model error
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()


# Predict using the test set
prediction <- predict(rf_model, cluster1test)

solution <- data.frame(cluster1test, subscribed = prediction)
solution

library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.6075

library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.5897, now it is 0.6075

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")


##################################### Decision Tree
#8373
row_count <- nrow(cluster2)
shuffled_rows <- sample(row_count)
cluster2train <- cluster2[head(shuffled_rows,floor(row_count*0.75)),]
cluster2test <- cluster2[tail(shuffled_rows,floor(row_count*0.25)),]

set.seed(1984)

library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2train,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

# Predict using the test set
prediction <- predict(dt_model, cluster2test)

solution <- data.frame(cluster2test, subscribed = prediction)
solution$subscribed <- ifelse(solution$subscribed.yes > solution$subscribed.no, "yes", "no")

solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#nrow(solution[solution$y=='no',])
#Balanced Accuracy: 0.51499

#library(pROC)
auc(as.numeric(solution$y), as.numeric(factor(solution$subscribed)))
#Area under the curve: 0.5109, now it is 0.515

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(factor(solution$subscribed))), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")


################################### Support vector machine (SVM)
cluster31he <- cluster3
#install.packages(ade4)
#One-hot-encoding features:
#library(ade4)
#library(data.table)
ohe_feats = c('job', 'marital', 'education', 'default', 'housing', 
             'loan', 'contact', 'month', 'day_of_week', 'poutcome')
for (f in ohe_feats){
  df_all_dummy = acm.disjonctif(cluster31he[f])
  cluster31he[f] = NULL
  cluster31he = cbind(cluster31he, df_all_dummy)
}
cluster31he
cluster31he$y <- ifelse(cluster31he$y == "yes", 1, 0)
cluster31he$y <- factor(cluster31he$y)

#remove duration as per UCI website
cluster31he$duration <- NULL
#remove cluster column 12
cluster31he <- cluster31he[,-12]
nrow(cluster31he[cluster31he$default.yes==1,]) #0
#remove default.yes column 38
cluster31he <- cluster31he[,-38]
nrow(cluster31he[cluster31he$education.illiterate==1,]) #4
#remove education.illiterate 32
cluster31he <- cluster31he[,-32]

#12472
row_count <- nrow(cluster31he)
shuffled_rows <- sample(row_count)
cluster3train <- cluster31he[head(shuffled_rows,floor(row_count*0.75)),]
cluster3test <- cluster31he[tail(shuffled_rows,floor(row_count*0.25)),]

#library(caret)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3233)

svm_Linear <- train(y ~ .,
                          data = cluster3train,
                          method = "svmLinear",
                          trControl=trctrl,
                          preProcess = c("center", "scale"),
                          tuneLength = 10)

# Predict using the test set
prediction <- predict(svm_Linear, newdata = cluster3test)
prediction

solution <- data.frame(cluster3test, subscribed = prediction)
solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)

#library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.5997

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")


################################# Naive Bayes
#4968
row_count <- nrow(cluster4)
shuffled_rows <- sample(row_count)
cluster4train <- cluster4[head(shuffled_rows,floor(row_count*0.75)),]
cluster4test <- cluster4[tail(shuffled_rows,floor(row_count*0.25)),]

library(e1071)

nb_model <- naiveBayes(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data=cluster4train)

# Predict using the test set
prediction <- predict(nb_model, cluster4test)

solution <- data.frame(cluster4test, subscribed = prediction)
solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.7160

#library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.716

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")

```

The higher the area under the curve (AUC), the better the prediction model.
We tried 4 classification models: Random Forest, Decision Tree, Support Vector Machine (SVM), and Naive Bayes.
The order of models from highest AUC to lowest is:
1. Naive Bayes    (AUC: 0.716)
2. SVM            (AUC: 0.5997)
3. Random Forest  (AUC: 0.5897)
4. Decision Tree  (AUC: 0.5109)