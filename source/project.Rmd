---
title: "ML 1000 - Project"
author: "Ali El-Sharif, Neha Panchal, Sagnik Adusumilli, Sarmad Shubber"
due date: "March 22, 2019"
output: word_document
---


## Abstract

TBD


## Background 

TBD


## Objective

TBD


# Data Analysis

Source of data: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

Extracted from https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#


## Data Dictionary

Column Name            | Column Description  
-----------------------| ------------------- 
age                    | Age of the customer
job                    | Type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
marital                | Marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed) 
education              | Education level of the customer*
default                | has credit in default(categorical: 'no','yes','unknown')
balance                | ??
housing                | has housing loan  (categorical: 'no','yes','unknown')
loan                   | has personal loan (yes/no)  
contract               | contact communication type (categorical: 'cellular','telephone') 
day                    | ??
month                  | month the customer was contrained previously in the year
duration               |  last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously 
campaign               |  number of contacts performed during this campaign and for this client (numeric, includes last contact)
pdays                  | number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
previous               | number of contacts performed before this campaign and for this client
poutcome               | outcome of the previous marketing campaign
y                      | has the client subscribed a term deposit? (binary: 'yes','no')

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#import packages;
#library(dplyr)
#library(reshape2)
#library(ggplot2)
#library(Hmisc)
#library(corrplot)
#library(mice)
#library(VIM)
#library(pROC)
#library(caret)
#library(sqldf)

# Clean all variables that might be left by other scripts
rm(list=ls(all=TRUE))

```

```{r bankData, include=FALSE}
# Read data
bankData = read.csv("../data/bank-additional-full.csv", header = TRUE, na.strings =  c("NA","","#NA"), sep=";")

head(bankData)
```


## Data Exploartion

Summary:
```{r summary, echo=FALSE}

summary(bankData)

```


Structure:
```{r structure, echo=FALSE}

str(bankData)

```


Creating a correlation matrix

```{r matrix, fig.width=10, fig.height=10, echo = FALSE, warning = FALSE, message = FALSE}
# we can only Create a correlation matrix with only numeric data, therefore we are going to use sapply to only get numeric data
# sapply is applying a fuction over list of vector
numericData <- bankData[sapply(bankData,is.numeric)]
matrix <- cor(numericData, use="pairwise.complete.obs")

library(corrplot)
corrplot(matrix, type = "lower", method = "circle", order="hclust", tl.srt = 45, tl.cex = 0.7)

```


## Missing Data
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Check missing values
sort(colSums(is.na(bankData)), decreasing = T)

```
Our dataset does not have any rows with columns that contain N/A.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#skip all the data with missing values (no missing values were found in this case)
#bankData <- na.omit(bankData)

```


```{r feature_distribution, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap=" Distribution of Important Features"}

library(dplyr)
tmp = bankData %>% filter(complete.cases(.))

library(RColorBrewer) # color palettes
mainPalette = brewer.pal(8,"Dark2") # pick palettes 

library(ggplot2)
p1 = tmp %>% ggplot(aes(x=age)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) #+ scale_x_log10()

p2 = tmp %>% ggplot(aes(x=duration)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) #+ scale_x_log10()

p3 = tmp %>% ggplot(aes(x=campaign)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2) #+ scale_x_log10()

p4 = tmp %>% ggplot(aes(x=pdays)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) #+ scale_x_log10()

p5 = tmp %>% ggplot(aes(x=previous)) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2) #+ scale_x_log10()

p6 = tmp %>% ggplot(aes(x=emp.var.rate)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2) #+ scale_x_log10()

p7 = tmp %>% ggplot(aes(x=cons.price.idx)) + geom_density(fill=mainPalette[7], colour=mainPalette[7], alpha = 0.2) #+ scale_x_log10()

p8 = tmp %>% ggplot(aes(x=cons.conf.idx)) + geom_density(fill=mainPalette[8], colour=mainPalette[8], alpha = 0.2) #+ scale_x_log10() # this plot looks strange

p9 = tmp %>% ggplot(aes(x=euribor3m)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2) #+ scale_x_log10()

p10 = tmp %>% ggplot(aes(x=nr.employed)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) #+ scale_x_log10()


library(gridExtra) # arrange grids
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10)
rm(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,tmp)
```

## Insights from Data explorations


 

## Data Preparation

```{r}
bankData1he <- bankData
#install.packages(ade4)
#One-hot-encoding of categorical features:
library(ade4)

categorical_columns = c('job', 'marital', 'education', 'default', 'housing', 
             'loan', 'contact', 'month', 'day_of_week', 'poutcome')
for (f in categorical_columns){
  df_all_dummy = acm.disjonctif(bankData1he[f])
  bankData1he[f] = NULL
  bankData1he = cbind(bankData1he, df_all_dummy)
}

str(bankData1he)
head(bankData1he)



# Remove y column (column 11) before scaling and clustering
#scale the variables
scaled_bd <- scale(bankData1he[,-c(11)])

#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
wss <- sapply(1:k.max, 
              function(k){kmeans(scaled_bd, k, nstart=50,iter.max = 15 )$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

# k = 3, from the above plot we picked the elbow to be k=3 meaning 3 clusters
library(cluster)
clarax <- clara(scaled_bd, 3, metric = "jaccard", stand = TRUE, 
      samples = 5, pamLike = TRUE)

clarax
# cluster sizes: 19144, 9339, 12705

bankDataWithCluster <- cbind(bankData, cluster = clarax$clustering)
#head(bankDataWithCluster, n = 4)

# Medoids
clarax$medoids
# The mediods are 4152, 21730, 13128

# plot the cluster solution
library(fpc)
plotcluster(scaled_bd, clarax$clustering)

# display data around the mediods for each cluster
bankDataWithCluster[4140:4160,]
bankDataWithCluster[21720:21740,]
bankDataWithCluster[13120:13140,]

cluster1 <- bankDataWithCluster[bankDataWithCluster$cluster==1,]
cluster2 <- bankDataWithCluster[bankDataWithCluster$cluster==2,]
cluster3 <- bankDataWithCluster[bankDataWithCluster$cluster==3,]

cluster1yes <- cluster1[cluster1$y=='yes',]
nrow(cluster1yes) #1725
cluster1no <- cluster1[cluster1$y=='no',]
nrow(cluster1no) #17419

cluster2yes <- cluster2[cluster2$y=='yes',]
nrow(cluster2yes) #1208
cluster2no <- cluster2[cluster2$y=='no',]
nrow(cluster2no) #8131

cluster3yes <- cluster3[cluster3$y=='yes',]
nrow(cluster3yes) #1707
cluster3no <- cluster3[cluster3$y=='no',]
nrow(cluster3no) #10998

```

 

## Data Imputing


 

## Feature Selection


 

## Modeling

To learn more about each cluster, we will run a Random Forest and get the important features (columns) of the cluster. We will also run a Decision Tree to know which are the most important features (columns).
```{r}
######################################## Random Forest on cluster1 - most important features
#19144 rows

# Set a random seed
set.seed(754)

library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster1)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: age, euribor3m
#3rd feature: job
#4th feature: nr.employed


##################################### Decision Tree on cluster1 - most important features
#19144 rows

set.seed(1984)

library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, poutcome, pdays



######################################## Random Forest on cluster2 - most important features
#9339 rows

# Set a random seed
set.seed(454)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: euribor3m, age
#3rd feature: job
#4th feature: nr.employed


##################################### Decision Tree on cluster2 - most important features
#9339 rows

set.seed(1484)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster2,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, poutcome, pdays



######################################## Random Forest on cluster3 - most important features
#12705 rows

# Set a random seed
set.seed(760)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster3)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()

#Top 2 features: age, euribor3m
#3rd feature: job
#4th feature: nr.employed


##################################### Decision Tree on cluster3 - most important features
#12705 rows

set.seed(1254)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = cluster3,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

summary(dt_model)

#variable importance: nr.employed, euribor3m, emp.var.rate, cons.conf.idx, cons.price.idx, month, pdays, poutcome, previous


########################### Supervised Learning - prediction models

######################################## Random Forest
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

# Set a random seed
set.seed(754)

#library(randomForest)
# Build the model (note: not all possible variables are used)
# remove duration column as per website recommendation
rf_model <- randomForest(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = train)

# Show model error
plot(rf_model, ylim=c(0,0.36))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)

# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

#library(ggthemes)
# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()


# Predict using the test set
prediction <- predict(rf_model, test)

solution <- data.frame(test, subscribed = prediction)
#solution

library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.6326

library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.6326

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")


##################################### Decision Tree
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

set.seed(1934)

#library(rpart)
dt_model <- rpart(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data = train,
                                            method = "class",
                                            minsplit = 2,
                                            minbucket = 1)
#library(rpart.plot)
rpart.plot(dt_model, extra=4) # plot tree

# Predict using the test set
prediction <- predict(dt_model, test)

solution <- data.frame(test, subscribed = prediction)
solution$subscribed <- ifelse(solution$subscribed.yes > solution$subscribed.no, "yes", "no")

#solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#nrow(solution[solution$y=='no',])
#Balanced Accuracy: 0.5914

#library(pROC)
auc(as.numeric(solution$y), as.numeric(factor(solution$subscribed)))
#Area under the curve: 0.5914

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(factor(solution$subscribed))), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")


################################# Naive Bayes
row_count <- nrow(bankDataWithCluster)
shuffled_rows <- sample(row_count)
train <- bankDataWithCluster[head(shuffled_rows,floor(row_count*0.75)),]
test <- bankDataWithCluster[tail(shuffled_rows,floor(row_count*0.25)),]

library(e1071)

nb_model <- naiveBayes(y ~ age + job + marital + education + default + 
                                            housing + loan + contact + 
                                            month + day_of_week +
                                            campaign + pdays + previous +
                                            poutcome + emp.var.rate + cons.price.idx +
                                            cons.conf.idx + euribor3m + nr.employed,
                                            data=train)

# Predict using the test set
prediction <- predict(nb_model, test)

solution <- data.frame(test, subscribed = prediction)
#solution

#library(caret)
confusionMatrix(factor(solution$subscribed), solution$y)
#Balanced Accuracy: 0.7129

#library(pROC)
auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.7129

roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col = "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")



################################### Support vector machine (SVM)
# SVM takes over 30 minutes to run so we are excluding it
##bankDataWithCluster1he <- bankDataWithCluster
#install.packages(ade4)
#One-hot-encoding of categorical features because SVM only accepts numerical columns:
#library(ade4)
#library(data.table)
##categorical_columns = c('job', 'marital', 'education', 'default', 'housing', 
##                        'loan', 'contact', 'month', 'day_of_week', 'poutcome')
##for (f in categorical_columns){
##  df_all_dummy = acm.disjonctif(bankDataWithCluster1he[f])
##  bankDataWithCluster1he[f] = NULL
##  bankDataWithCluster1he = cbind(bankDataWithCluster1he, df_all_dummy)
##}

##bankDataWithCluster1he$y <- ifelse(bankDataWithCluster1he$y == "yes", 1, 0)
##bankDataWithCluster1he$y <- factor(bankDataWithCluster1he$y)

#remove duration as per UCI website
##bankDataWithCluster1he$duration <- NULL

#remove cluster column 12
##bankDataWithCluster1he$cluster <- NULL

##nrow(bankDataWithCluster1he[bankDataWithCluster1he$default.yes==1,]) #3
#remove default.yes column 38 because it is nearly all 0's and SVM doesn't allow that
##bankDataWithCluster1he$default.yes <- NULL

##nrow(bankDataWithCluster1he[bankDataWithCluster1he$education.illiterate==1,]) #19
#remove education.illiterate column 32 because it is nearly all 0's and SVM doesn't allow that
##bankDataWithCluster1he$education.illiterate <- NULL

##row_count <- nrow(bankDataWithCluster1he)
##shuffled_rows <- sample(row_count)
##train <- bankDataWithCluster1he[head(shuffled_rows,floor(row_count*0.75)),]
##test <- bankDataWithCluster1he[tail(shuffled_rows,floor(row_count*0.25)),]

#library(caret)
##trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
##set.seed(3233)

##svm_Linear <- train(y ~ .,
##                          data = train,
##                          method = "svmLinear",
##                          trControl=trctrl,
##                          preProcess = c("center", "scale"),
##                          tuneLength = 10)

# Predict using the test set
##prediction <- predict(svm_Linear, newdata = test)
##prediction

##solution <- data.frame(test, subscribed = prediction)
##solution

#library(caret)
##confusionMatrix(factor(solution$subscribed), solution$y)

#library(pROC)
##auc(as.numeric(solution$y), as.numeric(solution$subscribed))
#Area under the curve: 0.5997

##roc_rose <- plot(roc(as.numeric(solution$y), as.numeric(solution$subscribed)), print.auc = TRUE, col ##= "blue", xlab = "False positive rate (Specificity)", ylab = "True positive rate (Sensitivity)")
```

The higher the area under the curve (AUC), the better the prediction model.
We tried 4 classification models: Random Forest, Decision Tree, Naive Bayes, and Support Vector Machine (SVM). SVM takes over 30 minutes to run so we are excluding its results.
The order of models from highest AUC to lowest is:
1. Naive Bayes    (AUC: 0.7129)
2. Random Forest  (AUC: 0.6326)
3. Decision Tree  (AUC: 0.5914)